<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mengying YU</title>
  
  <meta name="author" content="Mengying YU">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mengying YU</name>
              </p>
              <p>
                I am a perception engineer in  <a href="https://en.desaysv.com/">Desay SV Automotive Co., Ltd.</a> and focus on vision perception of autonomous vehicles, including BEV 3D detection, lane detection, traffic light detection.
              </p>
              <p>  
                I received my Master degree in Artificial Intelligence at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a> in 2023, and B.Eng in Computer Science with Artificial Intelligence at <a href="https://www.nottingham.edu.cn/en/index.aspx">University of Nottingham Ningbo China</a> in 2021.
              </p>
              <p>
                My primary research focus revolves around multimodal understanding and autonomous vehicles perception within the field of computer vision. 
              </p>
              <p style="text-align:center">
                <a href="mailto:yume0004@e.ntu.edu.sg">Email</a> &nbsp/&nbsp
                <a href="data\CV_YU_MENGYING.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?user=G8DPsoUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                <a href="linkedin.com/in/mengying-yu-070784222">LinkedIn</a> 
                <!-- <a href="https://github.com/GSeanCDAT">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:50%;max-width:50%">
              <a href="images/picture.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/picture.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Project</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="pvsg_stop()" onmouseover="pvsg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/driving.png' width="160" height="100">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://github.com/Luodian/RelateAnything"> -->
                <papertitle>Autonomous Vehicles Perception
                </papertitle>
              <!-- </a> -->
              <br>
              BEV 3D camera detection</br>
              2D/3D traffic light, traffic sign detection  (<button onclick="playLocalVideo(event)">播放视频</button>)</br>
              2D lane detection</br>
              All of these program can be deployed on vehicles and achieve good performance.
              <!-- <strong>Zujin Guo</strong>, Bo Li, Jingkang Yang, Zijian Zhou (alphabetical order) -->
              <br>
              <!-- <a href="https://github.com/Luodian/RelateAnything">Code</a>
              /
              <a href="https://bf5e65e511446cbe60.gradio.live/">Demo</a> -->
              <p></p>

            </td>
          </tr>
	  <div id="video-container" style="display: none;">
    		<video id="demo-video" width="800" height="450" controls>
        		<source src='images/TLTS.mp4' type='video/mp4'>
   		 </video>
	</div>

	<script>
	    // function playLocalVideo(event) {
	    //     event.preventDefault(); // 阻止默认行为
	
	    //     // 获取video元素
	    //     var videoElement = document.getElementById('demo-video');
	    //     var videoContainer = document.getElementById('video-container');
	        
	    //     videoElement.play();
	    //     videoContainer.style.display = 'block';
	    // }
	   function playLocalVideo(event) {
	        event.preventDefault(); // 阻止链接的默认行为
	
	        // 获取视频元素和其容器
	        var videoElement = document.getElementById('demo-video');
	        var videoContainer = document.getElementById('video-container');
	
	        // 添加 'loadeddata' 事件的事件监听器，该事件在视频加载完成时触发
	        videoElement.addEventListener('loadeddata', function () {
	            // 视频加载完成后播放
	            videoElement.play();
	        });
	
	        // 设置视频源
	        videoElement.src = 'images/TLTS.mp4';
	
	        // 显示视频容器
	        videoContainer.style.display = 'block';
	    }
	</script>


        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
              <!-- <p>
                I'm interested in computer vision and machine learning, especially in multimodal scenario's understanding and generation. 
                I also feel excited in exploring self-supervised learning methods for models on the massive data in the wild. Some of the work has been published.
                Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="pvsg_stop()" onmouseover="pvsg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/dataset.png' width="160" height="80">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf"> -->
                <papertitle>Dataset versus reality: Understanding model performance
                  from the perspective of information need
                </papertitle>
              </a>
              <br>
              <strong>Mengying Yu</strong>, Aixin Sun.
              <br>
              <em>JASIST</em>, 2023
              <br>
              <a href="https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24825">Paper</a>
              /
              Code
              <p></p>
              <!-- <p>
                The Panoptic Scene Graph Generation (PSG) Task aims to interpret a complex scene image with a scene graph representation, with each node in the scene graph grounded by its pixel-accurate segmentation mask in the image. We established the first PSG dataset together with two-stage and one-stage baselines for it.
              </p> -->
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="pvsg_stop()" onmouseover="pvsg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/BEV.jpg' width="160" height="109">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf"> -->
                <papertitle> Towards Efficient 3D Object Detection in Bird’s-Eye-
                  Space for Autonomous Driving: A Convolutional-Only Approch
                </papertitle>
              </a>
              <br>
              Yuxin Li, Qiang Han, <strong>Mengying Yu</strong>, Yuxin Jiang, Chai Kiat Yeo, Yiheng Li, Zihang Huang, Nini
              Liu, Hsuanhan Chen and Xiaojun Wu
              <br>
              <em>ITSC (Oral)</em>, 2023
              <br>
	      <a href="https://arxiv.org/abs/2312.00633">Paper</a>
              <!-- <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf">Paper</a>
              /
              Code
              <p></p>
              <!-- <p>
                The Panoptic Scene Graph Generation (PSG) Task aims to interpret a complex scene image with a scene graph representation, with each node in the scene graph grounded by its pixel-accurate segmentation mask in the image. We established the first PSG dataset together with two-stage and one-stage baselines for it.
              </p> --> 
            </td>
          </tr>

        </tbody></table>

				
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Code&Dataset</heading>
              
            </td>
          </tr>

        </tbody></table>
		
        <ul>
          <li><a href="https://psgdataset.org/"><strong>PSG</strong></a>: Dataset and Benchmark for Panoptic Scene Graph Generation (PSG), ECCV'22</li>
          <li><a href="https://github.com/Luodian/RelateAnything"><strong>RAM</strong></a>: Code for Relate Anything</li>
        </ul>
					
					
        </tbody></table>  -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last update: Oct 2023  &nbsp&nbsp&nbsp&nbsp <a href="https://github.com/jonbarron/website">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  <a href="https://clustrmaps.com/site/1bwvw" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=9JIsDINI1ffkhfw2rVXlSEgPwtWTnTlwEuPnG7lB_lg&cl=ffffff"></a>
</body>

</html>
